{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vengottip/CS598_practical_statistical_learning/blob/main/CS598_PSL_Project3_xgboost_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2f9256ef-bd66-4875-865a-6233a33b9964",
      "metadata": {
        "id": "2f9256ef-bd66-4875-865a-6233a33b9964"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "777b5502-f540-4084-8e83-9ca5dc04c7f3",
      "metadata": {
        "id": "777b5502-f540-4084-8e83-9ca5dc04c7f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99b09fc-4191-4cc7-d78e-d95ff3f99af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.10/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.5.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.24.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (11.0.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAQg00frx-CY",
        "outputId": "08680f13-1f1b-4262-bbb1-2cd1f8f095e7"
      },
      "id": "jAQg00frx-CY",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "ijS_ZbMAyDKg"
      },
      "id": "ijS_ZbMAyDKg",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pungUYqqzspJ",
        "outputId": "8365ac9f-2447-489c-d9bc-f8907f8dd06e"
      },
      "id": "pungUYqqzspJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvmhyLXX0cgl",
        "outputId": "1eb924eb-cade-4645-b470-1c67089cef39"
      },
      "id": "NvmhyLXX0cgl",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.data.path.append('/root/nltk_data')  # Update this to a valid path"
      ],
      "metadata": {
        "id": "C7lEvGA4z-Rc"
      },
      "id": "C7lEvGA4z-Rc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stop words\n",
        "# Ensure required NLTK data resources are downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'punkt' resource...\")\n",
        "    download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'stopwords' resource...\")\n",
        "    download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "KGxfcfcJyHXE"
      },
      "id": "KGxfcfcJyHXE",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if word.lower() not in stop_words])"
      ],
      "metadata": {
        "id": "IV3BTrLQyMg2"
      },
      "id": "IV3BTrLQyMg2",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_important_features(X_train, y_train, alpha=0.0001):\n",
        "    \"\"\"\n",
        "    Select important features using Lasso regression.\n",
        "\n",
        "    Parameters:\n",
        "        X_train (array): Training features.\n",
        "        y_train (array): Training labels.\n",
        "        alpha (float): Regularization strength for Lasso.\n",
        "\n",
        "    Returns:\n",
        "        array: Selected feature indices.\n",
        "    \"\"\"\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Apply Lasso\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    selected_features = np.where(lasso.coef_ != 0)[0]\n",
        "\n",
        "    print(f\"Number of features selected: {len(selected_features)}\")\n",
        "    return selected_features"
      ],
      "metadata": {
        "id": "3h94lhocyQeJ"
      },
      "id": "3h94lhocyQeJ",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_column(df, column_name):\n",
        "    \"\"\"\n",
        "    Remove stop words from a text column in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Input DataFrame.\n",
        "        column_name (str): Name of the text column to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Updated DataFrame with preprocessed text.\n",
        "    \"\"\"\n",
        "    df[column_name] = df[column_name].apply(remove_stopwords)\n",
        "    return df"
      ],
      "metadata": {
        "id": "o3i2wdMEySU7"
      },
      "id": "o3i2wdMEySU7",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_xgboost_with_split_number(\n",
        "    train_file, test_file, test_y_file, split_number, model_save_path=\"xgboost_model\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Train and evaluate an XGBoost model and save it with a suffix for the split number.\n",
        "\n",
        "    Parameters:\n",
        "        train_file (str): Path to the training CSV file.\n",
        "        test_file (str): Path to the test CSV file.\n",
        "        test_y_file (str): Path to the test labels CSV file.\n",
        "        split_number (int): The split number to use as a suffix for saving the model.\n",
        "        model_save_path (str): Base path for saving the model (suffix added automatically).\n",
        "\n",
        "    Returns:\n",
        "        float: AUC score of the model.\n",
        "    \"\"\"\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    test_y = pd.read_csv(test_y_file)\n",
        "\n",
        "    # Preprocess review text to remove stop words\n",
        "    train_data = preprocess_text_column(train_data, \"review\")\n",
        "    test_data = preprocess_text_column(test_data, \"review\")\n",
        "\n",
        "    # Extract features and labels\n",
        "    X_train = train_data.iloc[:, 3:].values  # Skip 'id', 'sentiment', and 'review'\n",
        "    y_train = train_data['sentiment'].values\n",
        "    X_test = test_data.iloc[:, 2:].values  # Skip 'id' and 'review'\n",
        "    y_test = test_y['sentiment'].values\n",
        "\n",
        "    # Impute missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train = imputer.fit_transform(X_train)\n",
        "    X_test = imputer.transform(X_test)\n",
        "\n",
        "    # Remove low-variance features\n",
        "    feature_variances = np.var(X_train, axis=0)\n",
        "    X_train = X_train[:, feature_variances > 1e-5]\n",
        "    X_test = X_test[:, feature_variances > 1e-5]\n",
        "\n",
        "    # Feature selection using Lasso\n",
        "    selected_features = select_important_features(X_train, y_train, alpha=0.0001)\n",
        "    if len(selected_features) == 0:\n",
        "        raise ValueError(\"No features were selected by Lasso. Adjust the regularization strength or check the dataset.\")\n",
        "    X_train = X_train[:, selected_features]\n",
        "    X_test = X_test[:, selected_features]\n",
        "\n",
        "    # Save selected features indices for later use\n",
        "    selected_features_file = f\"selected_features_{split_number}.npy\"\n",
        "    np.save(selected_features_file, selected_features)\n",
        "    print(f\"Selected features saved to {selected_features_file}\")\n",
        "\n",
        "    # Train the XGBoost classifier\n",
        "    model = XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='auc',\n",
        "        n_estimators=2300,\n",
        "        learning_rate=0.02,\n",
        "        max_depth=8,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.6,\n",
        "        reg_lambda=0.1,\n",
        "        reg_alpha=0.01,\n",
        "        gamma=0,\n",
        "        scale_pos_weight=3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the model with a suffix for the split number\n",
        "    model_file_path = f\"{model_save_path}_{split_number}.json\"\n",
        "    model.save_model(model_file_path)\n",
        "    print(f\"Model saved to {model_file_path}\")\n",
        "\n",
        "    # Predict probabilities and calculate AUC\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_prob)\n",
        "    print(f\"AUC Score: {auc_score:.5f}\")\n",
        "    print(f\"Number of selected features: {len(selected_features)}\")\n",
        "\n",
        "    # Save additional data needed for interpretation\n",
        "    try:\n",
        "        assert X_test.shape[0] == len(test_data[\"review\"]), \"Mismatch between X_test and test reviews.\"\n",
        "    except AssertionError as e:\n",
        "        print(f\"Assertion failed: {e}\")\n",
        "        return\n",
        "    np.save(f\"X_test_{split_number}.npy\", X_test)\n",
        "    np.save(f\"test_reviews_{split_number}.npy\", test_data[\"review\"].values)\n",
        "    return auc_score\n"
      ],
      "metadata": {
        "id": "St-NAqdjsYOS"
      },
      "id": "St-NAqdjsYOS",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loaded_features = np.load(\"selected_features_1.npy\")\n",
        "print(test_loaded_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcE7N58YiuRs",
        "outputId": "692b7bf3-0c0f-4887-8797-d90b1ed76fc0"
      },
      "id": "qcE7N58YiuRs",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    1    2 ... 1530 1531 1532]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import platform\n",
        "import psutil\n",
        "\n",
        "specs = {\n",
        "        \"OS\": platform.system() + \" \" + platform.release(),\n",
        "        \"Processor\": platform.processor(),\n",
        "        \"RAM\": f\"{round(psutil.virtual_memory().total / (1024**3), 2)} GB\",\n",
        "    }\n",
        "print (specs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfzOvGJN_46k",
        "outputId": "967aac9c-f4bc-43a9-d608-6aef50868a43"
      },
      "id": "NfzOvGJN_46k",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'OS': 'Linux 6.1.85+', 'Processor': 'x86_64', 'RAM': '83.48 GB'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import platform\n",
        "import psutil\n",
        "\n",
        "def get_system_specs():\n",
        "    \"\"\"\n",
        "    Retrieve system specifications (CPU, RAM, OS).\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of system specifications.\n",
        "    \"\"\"\n",
        "    specs = {\n",
        "        \"OS\": platform.system() + \" \" + platform.release(),\n",
        "        \"Processor\": platform.processor(),\n",
        "        \"RAM\": f\"{round(psutil.virtual_memory().total / (1024**3), 2)} GB\",\n",
        "    }\n",
        "    return specs\n",
        "\n",
        "def evaluate_all_splits_dynamic(base_dir=\".\", model_save_path=\"xgboost_model\"):\n",
        "    \"\"\"\n",
        "    Evaluate and save XGBoost models for all splits in the provided directory.\n",
        "\n",
        "    Parameters:\n",
        "        base_dir (str): Base directory containing split folders (e.g., split_1, split_2, etc.).\n",
        "        model_save_path (str): Base name for saving models (suffix with split number will be added).\n",
        "\n",
        "    Returns:\n",
        "        list: AUC scores for all splits.\n",
        "    \"\"\"\n",
        "    system_specs = get_system_specs()\n",
        "    print(\"\\nSystem Specifications:\")\n",
        "    for key, value in system_specs.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    total_start_time = time.time()  # Start total timer\n",
        "    auc_scores = []\n",
        "\n",
        "    for split_number in range(1, 6):  # Iterate through split_1 to split_5\n",
        "        split_dir = os.path.join(base_dir, f\"split_{split_number}\")\n",
        "        train_file = os.path.join(split_dir, \"train.csv\")\n",
        "        test_file = os.path.join(split_dir, \"test.csv\")\n",
        "        test_y_file = os.path.join(split_dir, \"test_y.csv\")\n",
        "\n",
        "        print(f\"\\nEvaluating Split {split_number}...\")\n",
        "\n",
        "        # Measure time for the current split\n",
        "        split_start_time = time.time()\n",
        "\n",
        "        # Train and evaluate the model, save results\n",
        "        auc = train_and_evaluate_xgboost_with_split_number(\n",
        "            train_file=train_file,\n",
        "            test_file=test_file,\n",
        "            test_y_file=test_y_file,\n",
        "            split_number=split_number,  # Pass split number dynamically\n",
        "            model_save_path=model_save_path  # Base name for saving the model\n",
        "        )\n",
        "        split_end_time = time.time()\n",
        "        split_execution_time = split_end_time - split_start_time\n",
        "\n",
        "        print(f\"Execution Time for Split {split_number}: {split_execution_time:.2f} seconds\")\n",
        "        auc_scores.append(auc)\n",
        "\n",
        "    total_end_time = time.time()  # End total timer\n",
        "    total_execution_time = total_end_time - total_start_time\n",
        "\n",
        "    print(\"\\nAUC Scores for all splits:\")\n",
        "    for i, auc in enumerate(auc_scores, 1):\n",
        "        print(f\"Split {i}: AUC = {auc:.5f}\")\n",
        "\n",
        "    avg_auc = sum(auc_scores) / len(auc_scores)\n",
        "    print(f\"\\nAverage AUC across all splits: {avg_auc:.5f}\")\n",
        "    print(f\"\\nTotal Execution Time: {total_execution_time:.2f} seconds\")\n",
        "\n",
        "    return auc_scores\n"
      ],
      "metadata": {
        "id": "B-9u613GtAuo"
      },
      "id": "B-9u613GtAuo",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation across all splits\n",
        "if __name__ == \"__main__\":\n",
        "    base_directory = \".\"  # Replace with your base directory containing split folders\n",
        "    evaluate_all_splits_dynamic(base_dir=base_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055f5fe4-64bd-4646-cc14-b51dd7b7599f",
        "id": "Fb3hocD3tYfm"
      },
      "execution_count": 22,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "System Specifications:\n",
            "OS: Linux 6.1.85+\n",
            "Processor: x86_64\n",
            "RAM: 83.48 GB\n",
            "\n",
            "Evaluating Split 1...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.628e+02, tolerance: 6.250e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features selected: 1395\n",
            "Selected features saved to selected_features_1.npy\n",
            "Model saved to xgboost_model_1.json\n",
            "AUC Score: 0.98600\n",
            "Number of selected features: 1395\n",
            "Execution Time for Split 1: 1109.97 seconds\n",
            "\n",
            "Evaluating Split 2...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.646e+02, tolerance: 6.250e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features selected: 1382\n",
            "Selected features saved to selected_features_2.npy\n",
            "Model saved to xgboost_model_2.json\n",
            "AUC Score: 0.98518\n",
            "Number of selected features: 1382\n",
            "Execution Time for Split 2: 1129.41 seconds\n",
            "\n",
            "Evaluating Split 3...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.520e+02, tolerance: 6.250e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features selected: 1385\n",
            "Selected features saved to selected_features_3.npy\n",
            "Model saved to xgboost_model_3.json\n",
            "AUC Score: 0.98525\n",
            "Number of selected features: 1385\n",
            "Execution Time for Split 3: 1143.96 seconds\n",
            "\n",
            "Evaluating Split 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.643e+02, tolerance: 6.250e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features selected: 1389\n",
            "Selected features saved to selected_features_4.npy\n",
            "Model saved to xgboost_model_4.json\n",
            "AUC Score: 0.98535\n",
            "Number of selected features: 1389\n",
            "Execution Time for Split 4: 1139.28 seconds\n",
            "\n",
            "Evaluating Split 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.643e+02, tolerance: 6.250e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features selected: 1389\n",
            "Selected features saved to selected_features_5.npy\n",
            "Model saved to xgboost_model_5.json\n",
            "AUC Score: 0.98535\n",
            "Number of selected features: 1389\n",
            "Execution Time for Split 5: 1138.78 seconds\n",
            "\n",
            "AUC Scores for all splits:\n",
            "Split 1: AUC = 0.98600\n",
            "Split 2: AUC = 0.98518\n",
            "Split 3: AUC = 0.98525\n",
            "Split 4: AUC = 0.98535\n",
            "Split 5: AUC = 0.98535\n",
            "\n",
            "Average AUC across all splits: 0.98543\n",
            "\n",
            "Total Execution Time: 5661.40 seconds\n"
          ]
        }
      ],
      "id": "Fb3hocD3tYfm"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXRwzObGkfF-"
      },
      "id": "DXRwzObGkfF-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}